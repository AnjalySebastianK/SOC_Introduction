# SOC Terminologies – Practical Glossary

This README gives you a **practical, SOC-focused glossary** you can actually use when reading playbooks, tuning rules, or writing IR documentation.  
Terms are grouped into:

- **Foundations:** How a SOC is structured and works  
- **Monitoring:** How data and activity are collected and prepared  
- **Detection:** How suspicious behavior is identified  
- **Response:** How incidents are handled and contained  
- **Reporting:** How results, metrics, and maturity are communicated  

---

## 1. SOC foundations

### 1.1 SOC terminology

**SOC (Security Operations Center)**  
A centralized function (team + processes + technology) responsible for **monitoring, detecting, responding to, and reporting on security events** across an organization’s IT/OT environment. It usually operates 24/7 and is measured on how quickly and effectively it can reduce risk and impact.

---

### 1.2 SOC components

**Core components typically include:**

- **People:** Analysts, engineers, incident responders, threat hunters, SOC manager, etc.  
- **Process:** Playbooks, SOPs, escalation paths, SLAs, communication rules.  
- **Technology:** SIEM, EDR/XDR, SOAR, UEBA, DLP, ticketing, threat intel, etc.  
- **Data sources:** Logs, telemetry, alerts, threat intelligence feeds.  

Together, these components form the **People–Process–Technology** triangle.

---

### 1.3 SOC workflow

A **SOC workflow** describes the end‑to‑end path from raw data to closed incident. Typical high‑level steps:

1. **Ingest:** Collect logs and telemetry from multiple sources.  
2. **Normalize & Correlate:** Standardize formats, enrich, and correlate events.  
3. **Detect:** Apply rules, analytics, and models to generate alerts.  
4. **Triage:** Prioritize alerts, decide if they are true/false positives.  
5. **Investigate:** Gather context, pivot across tools, determine impact and scope.  
6. **Respond:** Contain, eradicate, recover, and document actions.  
7. **Review & Improve:** Lessons learned, tuning, and process updates.

---

### 1.4 People, process, technology

**People**  
Human roles in the SOC: L1/L2/L3 analysts, incident responders, threat hunters, engineers, architects, SOC manager, etc. They bring **judgment, context, and decision‑making** that tools alone cannot provide.

**Process**  
Documented, repeatable ways of working: **playbooks, SOPs, escalation matrices, communication plans, SLAs**, and quality checks. Good processes reduce chaos and ensure consistent handling of incidents.

**Technology**  
Tools that support monitoring, detection, investigation, and response: **SIEM, EDR/XDR, SOAR, UEBA, DLP, firewalls, IDS/IPS, ticketing systems**, etc. Technology should **enable** people and processes, not replace them.

---

### 1.5 SOC maturity model

A **SOC maturity model** is a framework to assess how advanced and effective a SOC is. Levels often move from:

1. **Ad‑hoc / Reactive:** Minimal monitoring, mostly manual, respond after damage.  
2. **Basic:** Some SIEM, basic rules, limited coverage, inconsistent processes.  
3. **Defined:** Documented processes, better coverage, regular reporting.  
4. **Managed:** Metrics‑driven, continuous improvement, integrated tools.  
5. **Optimized / Proactive:** Threat hunting, automation, strong analytics, business‑aligned.

It helps plan **roadmaps** and justify investments.

---

### 1.6 SOC generations

Often described as **“SOC 1.0, 2.0, 3.0”** (naming varies):

- **SOC 1.0:** Log‑centric, SIEM‑only, mostly rule‑based, reactive.  
- **SOC 2.0:** Integrates endpoint, network, cloud, and threat intel; more correlation and context.  
- **SOC 3.0 (Modern SOC):** Heavy use of **automation, SOAR, AI/ML, XDR**, proactive hunting, and strong business alignment.

The idea: SOCs evolve from **simple log monitoring** to **intelligent, automated, and proactive defense**.

---

### 1.7 NOC vs SOC

**NOC (Network Operations Center)**  
Focus: **Availability, performance, and reliability** of IT services and networks. They care about uptime, latency, bandwidth, outages.

**SOC (Security Operations Center)**  
Focus: **Confidentiality, integrity, and security** of systems and data. They care about threats, intrusions, data loss, and abuse.

In short: **NOC = “keep it running”**, **SOC = “keep it safe”**. They must collaborate closely.

---

## 2. Monitoring

### 2.1 Log

A **log** is a record of an event or action generated by systems, applications, devices, or security tools.  
Examples: authentication logs, firewall logs, DNS logs, EDR telemetry. Logs are the **raw material** for monitoring, detection, and forensics.

---

### 2.2 Event vs incident

**Event**  
Any observable occurrence in a system or network. Most events are **benign** (e.g., a user login, a file access, a DNS query).

**Incident**  
A **security event or series of events** that has been confirmed to **pose a threat** to confidentiality, integrity, or availability (e.g., ransomware execution, data exfiltration, unauthorized access).

> Every incident is made of events, but not every event becomes an incident.

---

### 2.3 Event ID

An **Event ID** is a unique identifier assigned to a specific type of event by a system or application (e.g., Windows Event ID 4624 for successful logon).  
SOC analysts use Event IDs to **filter, search, and correlate** specific behaviors quickly.

---

### 2.4 Log normalization

**Log normalization** is the process of converting logs from different sources and formats into a **standard, consistent schema** (e.g., common fields like `src_ip`, `dst_ip`, `username`, `action`).  

Benefits:

- Easier correlation across tools and platforms  
- More reliable detection rules  
- Simpler queries and dashboards  

---

### 2.5 Log correlation

**Log correlation** links multiple events from different sources into a **single, meaningful story**.  

Example:  
- VPN login from unusual country  
- Followed by multiple failed logins  
- Followed by a successful login and privilege escalation  

Correlation rules or analytics combine these into **one alert** indicating possible account compromise.

---

## 3. Detection

### 3.1 Alert

An **alert** is a notification generated by a security tool (SIEM, EDR, IDS, etc.) when certain conditions or rules are met.  
Alerts are **signals** that something may be wrong and require triage. Not all alerts are incidents.

---

### 3.2 Alert fatigue

**Alert fatigue** occurs when analysts are overwhelmed by **too many alerts**, many of which are low‑value or false positives.  

Consequences:

- Important alerts get missed or delayed  
- Analyst burnout and reduced quality of investigation  
- Slower response and higher risk  

Reducing alert fatigue is a key SOC goal (through tuning, prioritization, and automation).

---

### 3.3 Alert verdict

The **alert verdict** is the final decision about an alert after triage and investigation, such as:

- **True Positive (TP):** Real malicious activity  
- **False Positive (FP):** Benign activity incorrectly flagged  
- **Benign True (BT):** Expected but unusual behavior  
- **Informational:** No action needed, but useful context  

The verdict drives whether an alert becomes an **incident** and what response is triggered.

---

### 3.4 True positive, false positive, true negative, false negative

**True Positive (TP)**  
The system correctly raises an alert for **actual malicious activity**.

**False Positive (FP)**  
The system raises an alert for **benign activity**. This wastes time and contributes to alert fatigue.

**True Negative (TN)**  
The system correctly **does not alert** when activity is benign.

**False Negative (FN)**  
The system **fails to alert** on actual malicious activity. This is dangerous because attacks go undetected.

A good SOC aims to **maximize TP and TN**, while **minimizing FP and FN**.

---

### 3.5 Tuning

**Tuning** is the process of adjusting detection rules, thresholds, and configurations to:

- Reduce **false positives**  
- Improve **true positives**  
- Align alerts with the organization’s environment and risk profile  

Examples: whitelisting known safe IPs, adjusting thresholds for login failures, refining correlation rules.

---

### 3.6 IOC (Indicator of Compromise)

An **IOC** is a piece of evidence that strongly suggests a system has been **compromised**.  

Examples:

- Known malicious IP or domain  
- File hash of malware  
- Suspicious registry key or process name  

IOCs are often used in **detection rules, threat hunting, and retroactive searches**.

---

### 3.7 IOA (Indicator of Attack)

An **IOA** focuses on **attacker behavior and intent**, not just artifacts. It describes **how** an attack is carried out.  

Examples:

- Unusual use of administrative tools (e.g., `PsExec`, `PowerShell` abuse)  
- Lateral movement patterns  
- Credential dumping behavior  

IOAs are more **resilient** than IOCs because they detect **tactics and techniques**, not just specific signatures.

---

### 3.8 Correlation / correlation rule

**Correlation** in detection means combining multiple events or conditions to identify **complex or multi‑stage attacks**.  

A **correlation rule** is a defined logic that triggers an alert when certain patterns occur, such as:

- Multiple failed logins followed by a successful login from the same IP  
- New admin account creation followed by mass file access  

Correlation rules help detect **advanced attacks** that single events would not reveal.

---

### 3.9 SIEM (Security Information and Event Management)

A **SIEM** is a central platform that:

- **Ingests and normalizes logs** from many sources  
- Provides **search, dashboards, and reporting**  
- Runs **detection rules and correlation**  
- Often integrates with **SOAR** and ticketing tools  

It is usually the **core monitoring and detection hub** of a SOC.

---

### 3.10 EDR (Endpoint Detection and Response)

**EDR** focuses on **endpoint telemetry** (workstations, servers, laptops):

- Monitors processes, file changes, registry, network connections  
- Detects suspicious behavior on endpoints  
- Provides tools for **investigation and response** (e.g., isolate host, kill process)

EDR is crucial for detecting **post‑compromise activity**.

---

### 3.11 XDR (Extended Detection and Response)

**XDR** extends the EDR concept across **multiple domains**:

- Endpoints, network, email, identity, cloud, etc.  
- Correlates signals across these layers  
- Provides unified detection, investigation, and response workflows  

Goal: **break silos** and give analysts a **single, correlated view** of attacks.

---

### 3.12 DLP (Data Loss Prevention)

**DLP** tools monitor and control **data movement** to prevent **unauthorized disclosure or exfiltration**.  

Examples:

- Blocking sensitive files from being emailed externally  
- Detecting credit card numbers in outbound traffic  
- Monitoring USB transfers  

DLP is detection + prevention focused on **data protection**.

---

### 3.13 UEBA (User and Entity Behavior Analytics)

**UEBA** uses analytics and machine learning to model **normal behavior** of users and entities (devices, applications) and detect **anomalies**.  

Examples:

- User logging in from unusual locations or at unusual times  
- Sudden spike in file access  
- Service account behaving like an interactive user  

UEBA is powerful for detecting **insider threats, account compromise, and subtle misuse**.

---

### 3.14 TTP (Tactics, Techniques, and Procedures)

**TTPs** describe **how attackers operate**:

- **Tactics:** High‑level goals (e.g., persistence, lateral movement)  
- **Techniques:** Ways to achieve those goals (e.g., scheduled tasks, pass‑the‑hash)  
- **Procedures:** Concrete implementations and tools used  

Frameworks like **MITRE ATT&CK** organize TTPs. SOCs map detections and incidents to TTPs to understand **coverage and gaps**.

---

## 4. Response

### 4.1 Playbook

A **playbook** is a **step‑by‑step, semi‑structured procedure** for handling a specific type of alert or incident.  

Example: “Suspicious login from foreign country” playbook might include:

- Validate user activity  
- Check geolocation and device fingerprint  
- Force password reset if needed  
- Document findings and close ticket  

Playbooks can be **manual, semi‑automated, or fully automated** (via SOAR).

---

### 4.2 Runbook

A **runbook** is often more **technical and detailed** than a playbook, focusing on **how** to execute specific tasks.  

Examples:

- Exact commands to collect memory from a host  
- Steps to block an IP on a specific firewall  
- Procedures to restore from backup  

Think of **playbook = “what & when”**, **runbook = “how exactly”**.

---

### 4.3 SOP (Standard Operating Procedure)

An **SOP** is a formal, approved document describing **standard, repeatable ways of working** in the SOC.  

Examples:

- SOP for incident classification and severity  
- SOP for communication with management during major incidents  
- SOP for evidence handling and chain of custody  

SOPs ensure **consistency, compliance, and auditability**.

---

### 4.4 SOAR (Security Orchestration, Automation, and Response)

**SOAR** platforms:

- Integrate with multiple tools (SIEM, EDR, firewalls, ticketing, email, etc.)  
- **Automate** repetitive tasks (enrichment, containment actions, notifications)  
- Orchestrate **playbooks** across systems  

Goal: reduce **MTTD/MTTR**, minimize manual work, and standardize response.

---

### 4.5 MTTR (Mean Time to Respond/Recover)

**MTTR** measures the **average time** it takes from **detecting an incident** to **fully responding/recovering** (containment, eradication, and restoration).  

Lower MTTR = **faster recovery**, less damage, better SOC performance.

---

### 4.6 MTTD (Mean Time to Detect)

**MTTD** measures the **average time** between the **start of malicious activity** and its **detection** by the SOC.  

Lower MTTD = **faster detection**, shorter attacker dwell time, reduced impact.

---

## 5. Reporting

### 5.1 SOC reporting

**SOC reporting** includes:

- **Operational reports:** Daily/weekly summaries of alerts, incidents, and actions.  
- **Management reports:** Trends, risk reduction, major incidents, resource needs.  
- **Compliance reports:** Evidence for audits, regulatory requirements, SLAs.  

Good reporting translates **technical activity into business‑relevant insight**.

---

### 5.2 Metrics and KPIs

Common SOC **metrics/KPIs** include:

- Number of alerts and incidents by severity  
- **MTTD, MTTR**  
- False positive rate  
- Coverage of critical assets and log sources  
- Progress along the **SOC maturity model**

These metrics show whether the SOC is **improving, stagnating, or regressing**.

---

## 6. Quick category mapping

- **Monitoring:** Log, event, event ID, log normalization, log correlation, SIEM  
- **Detection:** Alert, alert fatigue, alert verdict, TP/FP/TN/FN, tuning, IOC, IOA, correlation rule, EDR, XDR, DLP, UEBA, TTP  
- **Response:** Playbook, runbook, SOP, SOAR, MTTR  
- **Reporting:** SOC reporting, metrics/KPIs, SOC maturity model  
- **Foundations:** SOC terminology, components, workflow, people, process, technology, SOC generations, NOC vs SOC, MTTD
